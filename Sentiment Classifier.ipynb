{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqdlAmx2MehlUtSAF53dH8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKNIglJq4Yi5","executionInfo":{"status":"ok","timestamp":1763965572267,"user_tz":-180,"elapsed":21268,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"5cb902ed-2abe-4ed5-d771-a63c419cd6d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Environment ready!\n"]}],"source":["!pip install nltk scikit-learn\n","\n","# Import libraries\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","print(\"Environment ready!\")\n"]},{"cell_type":"code","source":["# STEP 2: Create a small simple sentiment dataset\n","\n","texts = [\n","    \"I love this movie, it was amazing!\",\n","    \"This film was terrible, I hated it.\",\n","    \"What a great experience, I enjoyed every moment.\",\n","    \"This is the worst thing I have ever watched.\",\n","    \"Absolutely fantastic! Highly recommended.\",\n","    \"Really boring and bad acting.\",\n","    \"I am so happy with this product!\",\n","    \"I am very disappointed, this was a waste of time.\",\n","    \"This made my day, I feel wonderful!\",\n","    \"I regret buying this, very poor quality.\"\n","]\n","\n","labels = [\n","    1,  # Positive\n","    0,  # Negative\n","    1,  # Positive\n","    0,  # Negative\n","    1,  # Positive\n","    0,  # Negative\n","    1,  # Positive\n","    0,  # Negative\n","    1,  # Positive\n","    0   # Negative\n","]\n","\n","print(\"Dataset loaded! Total samples:\", len(texts))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23VeNUrb46aU","executionInfo":{"status":"ok","timestamp":1763965572283,"user_tz":-180,"elapsed":12,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"fdde305c-e7da-49ea-c3dc-0897430d277b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded! Total samples: 10\n"]}]},{"cell_type":"code","source":["# STEP 3: Basic text cleaning\n","\n","import re\n","\n","def clean_text(text):\n","    # 1. Lowercase\n","    text = text.lower()\n","\n","    # 2. Remove punctuation and special characters\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # 3. Remove extra spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text\n","\n","# Apply cleaning to all texts\n","cleaned_texts = [clean_text(t) for t in texts]\n","\n","cleaned_texts\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7cnok8q7Lve","executionInfo":{"status":"ok","timestamp":1763965572296,"user_tz":-180,"elapsed":11,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"ede750a8-08d2-42e7-b0e3-4ae741a4d100"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i love this movie it was amazing',\n"," 'this film was terrible i hated it',\n"," 'what a great experience i enjoyed every moment',\n"," 'this is the worst thing i have ever watched',\n"," 'absolutely fantastic highly recommended',\n"," 'really boring and bad acting',\n"," 'i am so happy with this product',\n"," 'i am very disappointed this was a waste of time',\n"," 'this made my day i feel wonderful',\n"," 'i regret buying this very poor quality']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# STEP 4: Tokenization + Stopword removal + Lemmatization\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","print(\"All required NLTK data downloaded!\")\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess(text):\n","    # 1. Tokenize\n","    tokens = word_tokenize(text)\n","\n","    # 2. Remove stopwords\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # 3. Lemmatize\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    return tokens\n","\n","processed_texts = [preprocess(t) for t in cleaned_texts]\n","\n","processed_texts\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rq1xmcvo70lj","executionInfo":{"status":"ok","timestamp":1763965580560,"user_tz":-180,"elapsed":8262,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"4cb8be29-6869-4180-a0d7-c1a3edd5bd5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["All required NLTK data downloaded!\n"]},{"output_type":"execute_result","data":{"text/plain":["[['love', 'movie', 'amazing'],\n"," ['film', 'terrible', 'hated'],\n"," ['great', 'experience', 'enjoyed', 'every', 'moment'],\n"," ['worst', 'thing', 'ever', 'watched'],\n"," ['absolutely', 'fantastic', 'highly', 'recommended'],\n"," ['really', 'boring', 'bad', 'acting'],\n"," ['happy', 'product'],\n"," ['disappointed', 'waste', 'time'],\n"," ['made', 'day', 'feel', 'wonderful'],\n"," ['regret', 'buying', 'poor', 'quality']]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#  convert tokens back to text (TF-IDF expects text, not tokens)\n","joined_texts = [\" \".join(tokens) for tokens in processed_texts]\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create the TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the text data\n","X = vectorizer.fit_transform(joined_texts)\n","\n","X.toarray()[:5]  # Show first 5 vectors\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8laSy0_8Qlg","executionInfo":{"status":"ok","timestamp":1763965580567,"user_tz":-180,"elapsed":11,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"b0bc5761-411f-484a-c223-4d2af41b55e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.57735027, 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.57735027,\n","        0.        , 0.        , 0.57735027, 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.57735027,\n","        0.        , 0.        , 0.57735027, 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.57735027,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.4472136 , 0.        ,\n","        0.4472136 , 0.4472136 , 0.        , 0.        , 0.        ,\n","        0.4472136 , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.4472136 , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.5       ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.5       , 0.        , 0.        , 0.5       , 0.        ,\n","        0.5       ],\n","       [0.5       , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.5       , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.5       , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.5       , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Train a Machine Learning Model\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n","\n","# Create the model\n","model = MultinomialNB()\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","accuracy\n","## accuracy is low due low dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sf3YNoSi8vA5","executionInfo":{"status":"ok","timestamp":1763965580607,"user_tz":-180,"elapsed":37,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"5ac15437-45e4-4c9a-c542-9a378a55b7f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3333333333333333"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Test the model with custom sentences\n","\n","def predict_sentiment(text):\n","    # 1. Clean\n","    text = clean_text(text)\n","    # 2. Preprocess\n","    tokens = preprocess(text)\n","    # 3. Join back to string\n","    text_processed = \" \".join(tokens)\n","    # 4. Transform with TF-IDF\n","    X_input = vectorizer.transform([text_processed])\n","    # 5. Predict\n","    prediction = model.predict(X_input)[0]\n","\n","    return \"Positive \" if prediction == 1 else \"Negative \"\n","\n","# Try som sentences\n","print(predict_sentiment(\"I really loved this movie!\"))\n","print(predict_sentiment(\"This is the worst thing ever.\"))\n","print(predict_sentiment(\"I feel so happy today!\"))\n","print(predict_sentiment(\"I am very disappointed.\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFM50Hfq87NC","executionInfo":{"status":"ok","timestamp":1763965580636,"user_tz":-180,"elapsed":25,"user":{"displayName":"Osama Abdelaziz","userId":"14874879248825649500"}},"outputId":"41dce428-409f-46c0-b5ac-4a1864e49c3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive \n","Negative \n","Positive \n","Negative \n"]}]}]}